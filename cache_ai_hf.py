"""
Hugging Face Cache Builder Script
---------------------------------
Táº¡o cache AI cho cÃ¡c cÃ¢u há»i AWS SAA-C03 sá»­ dá»¥ng Hugging Face Inference API.
Model máº·c Ä‘á»‹nh: mistralai/Mistral-7B-Instruct-v0.3

CÃ¡ch sá»­ dá»¥ng:
    python cache_ai_hf.py 1-10           # Cache cÃ¢u há»i tá»« 1 Ä‘áº¿n 10
    python cache_ai_hf.py 1-10 --lang en # Cache cho tiáº¿ng Anh
    python cache_ai_hf.py 1-10 --force   # Ghi Ä‘Ã¨ cache cÅ©

YÃªu cáº§u:
    pip install httpx huggingface_hub python-dotenv
"""

import os
import sys
import argparse
import time
from typing import Optional
from dotenv import load_dotenv
import httpx

try:
    from huggingface_hub import InferenceClient
except ImportError:
    print("âŒ Error: Module 'huggingface_hub' chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t.")
    print("   Vui lÃ²ng cháº¡y: pip install huggingface_hub")
    sys.exit(1)

# Load environment variables
load_dotenv()

# Configuration
SUPABASE_URL = os.getenv('VITE_SUPABASE_URL') or os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('VITE_SUPABASE_ANON_KEY') or os.getenv('SUPABASE_KEY')

# Hugging Face Configuration
HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')
# Recommended models for Inference API (Free Tier friendly but powerful):
# - Qwen/Qwen2.5-Coder-32B-Instruct (Excellent for technical content, might be slower)
# - google/gemma-2-27b-it (Strong reasoning)
# - meta-llama/Meta-Llama-3-8B-Instruct (Fast, reliable)
HF_MODEL = os.getenv('HF_MODEL') or "Qwen/Qwen2.5-72B-Instruct"

# Validate configuration
if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ Error: SUPABASE_URL vÃ  SUPABASE_KEY chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh!")
    sys.exit(1)

if not HUGGINGFACE_API_KEY:
    print("âŒ Error: HUGGINGFACE_API_KEY chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh trong file .env!")
    sys.exit(1)

print(f"ğŸ”‘ Using Hugging Face API Key: {HUGGINGFACE_API_KEY[:4]}...{HUGGINGFACE_API_KEY[-4:]}")
print(f"ğŸ¤– Model: {HF_MODEL}")

# Supabase REST API headers
HEADERS = {
    'apikey': SUPABASE_KEY,
    'Authorization': f'Bearer {SUPABASE_KEY}',
    'Content-Type': 'application/json',
    'Prefer': 'return=minimal'
}


def get_theory_prompt(question: str, options: str, language: str) -> str:
    """Táº¡o prompt cho LÃ½ Thuyáº¿t (Theory)"""
    language_instruction = 'Vui lÃ²ng tráº£ lá»i báº±ng tiáº¿ng Viá»‡t.' if language == 'vi' else 'Please respond in English.'
    
    prompt_structure = """## CÆ¡ sá»Ÿ lÃ½ thuyáº¿t cÃ¡c thuáº­t ngá»¯ trong cÃ¢u há»i

Liá»‡t kÃª vÃ  giáº£i thÃ­ch Táº¤T Cáº¢ cÃ¡c AWS services, concepts, vÃ  thuáº­t ngá»¯ ká»¹ thuáº­t Ä‘Æ°á»£c Ä‘á» cáº­p trong cÃ¢u há»i.

Äá»‹nh dáº¡ng cho má»—i thuáº­t ngá»¯:
- **TÃªn thuáº­t ngá»¯** (in Ä‘áº­m, khÃ´ng cÃ³ dáº¥u hai cháº¥m)
- Giáº£i thÃ­ch ngáº¯n gá»n vÃ  Ä‘áº§y Ä‘á»§ vá» thuáº­t ngá»¯ Ä‘Ã³ (trÃªn dÃ²ng má»›i)

## CÆ¡ sá»Ÿ lÃ½ thuyáº¿t cÃ¡c thuáº­t ngá»¯ trong Ä‘Ã¡p Ã¡n

Liá»‡t kÃª vÃ  giáº£i thÃ­ch Táº¤T Cáº¢ cÃ¡c AWS services, concepts, vÃ  thuáº­t ngá»¯ ká»¹ thuáº­t xuáº¥t hiá»‡n trong cÃ¡c Ä‘Ã¡p Ã¡n (A, B, C, D).

Äá»‹nh dáº¡ng cho má»—i thuáº­t ngá»¯:
- **TÃªn thuáº­t ngá»¯** (in Ä‘áº­m, khÃ´ng cÃ³ dáº¥u hai cháº¥m)
- Giáº£i thÃ­ch ngáº¯n gá»n vÃ  Ä‘áº§y Ä‘á»§ vá» thuáº­t ngá»¯ Ä‘Ã³ (trÃªn dÃ²ng má»›i)

QUAN TRá»ŒNG: KHÃ”NG dÃ¹ng dáº¥u hai cháº¥m (:) sau tÃªn thuáº­t ngá»¯.""" if language == 'vi' else """## Theoretical Foundation of Question Terms

List and explain ALL AWS services, concepts, and technical terms mentioned in the question.

Format for each term:
- **Term name** (bold, NO colon)
- Concise but thorough explanation (on new line)

## Theoretical Foundation of Answer Terms

List and explain ALL AWS services, concepts, and technical terms appearing in the answers (A, B, C, D).

Format for each term:
- **Term name** (bold, NO colon)
- Concise but thorough explanation (on new line)

IMPORTANT: Do NOT use colons (:) after term names."""
    
    return f"""You are an AWS Solutions Architect expert. Provide theoretical foundation for this question.

Question: {question}

Options:
{options}

{language_instruction}

IMPORTANT: Start directly with the theoretical content. Do NOT include any greetings, introductions, or conclusions. Go straight to the structured content below.

Provide a comprehensive theoretical breakdown:

{prompt_structure}

Keep the theory organized and easy to reference (max 500 words)."""


def get_explanation_prompt(question: str, options: str, correct_answer: str, language: str) -> str:
    """Táº¡o prompt cho Giáº£i thÃ­ch (Explanation)"""
    language_instruction = 'Vui lÃ²ng tráº£ lá»i báº±ng tiáº¿ng Viá»‡t.' if language == 'vi' else 'Please respond in English.'
    
    prompt_structure = f"""## Giáº£i thÃ­ch cÃ¢u há»i

PhÃ¢n tÃ­ch yÃªu cáº§u chÃ­nh cá»§a cÃ¢u há»i, xÃ¡c Ä‘á»‹nh cÃ¡c Ä‘iá»ƒm máº¥u chá»‘t cáº§n chÃº Ã½.

## Giáº£i thÃ­ch Ä‘Ã¡p Ã¡n Ä‘Ãºng

Táº¡i sao Ä‘Ã¡p Ã¡n {correct_answer} lÃ  Ä‘Ãºng? Giáº£i thÃ­ch chi tiáº¿t.

## Táº¡i sao khÃ´ng chá»n cÃ¡c Ä‘Ã¡p Ã¡n khÃ¡c

PhÃ¢n tÃ­ch tá»«ng Ä‘Ã¡p Ã¡n sai, giáº£i thÃ­ch lÃ½ do.

## CÃ¡c lá»—i thÆ°á»ng gáº·p

Liá»‡t kÃª cÃ¡c lá»—i mÃ  thÃ­ sinh hay máº¯c pháº£i.

## Máº¹o Ä‘á»ƒ nhá»›

Cung cáº¥p cÃ¡c máº¹o, tricks Ä‘á»ƒ Ã¡p dá»¥ng cho cÃ¡c cÃ¢u há»i tÆ°Æ¡ng tá»±.

QUAN TRá»ŒNG: Khi Ä‘á» cáº­p Ä‘áº¿n cÃ¡c keywords hoáº·c concepts trong ná»™i dung, viáº¿t chÃºng á»Ÿ dáº¡ng **in Ä‘áº­m** KHÃ”NG CÃ“ dáº¥u hai cháº¥m (:) phÃ­a sau. VÃ­ dá»¥: **Keyword** chá»© khÃ´ng pháº£i **Keyword:**""" if language == 'vi' else f"""## Question Analysis

Analyze the main requirements of the question and identify the key points.

## Correct Answer Explanation

Why is answer {correct_answer} correct? Explain in detail.

## Why Other Answers Are Wrong

Analyze each incorrect answer and explain why.

## Common Mistakes

List the mistakes students often make.

## Tips to Remember

Provide tips and tricks to apply to similar questions.

IMPORTANT: When mentioning keywords or concepts in content, write them in **bold** withOUT colons (:) after. Example: **Keyword** NOT **Keyword:**"""
    
    return f"""You are an AWS Solutions Architect expert. Analyze this SAA-C03 exam question.

Question: {question}

Options:
{options}

Correct Answer: {correct_answer}

{language_instruction}

IMPORTANT: Start directly with the analysis. Do NOT include any greetings, introductions, or conclusions. Go straight to the structured content.

Do NOT use colons (:) after bold keywords. Write descriptions on the same line or new line without colons.

Provide a comprehensive explanation:

{prompt_structure}

Keep the explanation structured and easy to understand (max 500 words)."""

def call_huggingface(prompt: str, max_retries: int = 5) -> Optional[str]:
    """Gá»i Hugging Face API vá»›i retry logic cao hÆ¡n cho Serverless endpoints"""
    client = InferenceClient(api_key=HUGGINGFACE_API_KEY)

    for attempt in range(max_retries):
        try:
            # Try chat completion first (best for Instruction/Chat models)
            try:
                messages = [
                    {"role": "system", "content": "You are a helpful AWS expert assistant."},
                    {"role": "user", "content": prompt}
                ]
                
                response = client.chat_completion(
                    model=HF_MODEL,
                    messages=messages,
                    max_tokens=1500,
                    temperature=0.7
                )
                return response.choices[0].message.content
            
            except Exception as e:
                error_msg = str(e).lower()
                # If model doesn't support chat, fallback to text generation
                # "mn-404" often indicates model not found or endpoint issue which might be temporary or real
                if "not a chat model" in error_msg or "invalid_request_error" in error_msg or "mn-404" in error_msg:
                    print(f"   â„¹ï¸ Fallback to text generation (Chat API error: {e})...")
                    
                    # Manual basic formatting - Attempting generic Instruct format
                    formatted_prompt = f"{prompt}" 
                    
                    response = client.text_generation(
                        formatted_prompt,
                        model=HF_MODEL,
                        max_new_tokens=1500,
                        temperature=0.7
                    )
                    return response
                raise e # Re-raise if it's not a known fallback-able error

        except Exception as e:
            error_str = str(e).lower()
            
            # Handling model loading (503) or rate limits (429)
            if '503' in error_str or 'loading' in error_str:
                print(f"   â³ Model is loading... waiting longer (Attempt {attempt + 1})")
                time.sleep(10) # Wait longer for model load
                continue
                
            print(f"   âš ï¸ Attempt {attempt + 1} failed: {repr(e)}") # Use repr for more detail
            
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 3
                time.sleep(wait_time)
    
    print("   âŒ Hugging Face API call failed after retries")
    return None


def get_cached_content(question_id: str, language: str, content_type: str) -> Optional[str]:
    """Kiá»ƒm tra cache Ä‘Ã£ tá»“n táº¡i chÆ°a"""
    try:
        url = f"{SUPABASE_URL}/rest/v1/ai_cache"
        params = {
            'question_id': f'eq.{question_id}',
            'language': f'eq.{language}',
            'type': f'eq.{content_type}',
            'select': 'content'
        }
        
        with httpx.Client() as client:
            response = client.get(url, headers=HEADERS, params=params)
            if response.status_code == 200:
                data = response.json()
                if data and len(data) > 0:
                    return data[0]['content']
            return None
    except Exception as e:
        print(f"   âš ï¸ Cache check error: {e}")
        return None


def save_to_cache(question_id: str, language: str, content_type: str, content: str) -> bool:
    """LÆ°u káº¿t quáº£ vÃ o Supabase cache (delete + insert)"""
    try:
        url = f"{SUPABASE_URL}/rest/v1/ai_cache"
        
        with httpx.Client() as client:
            # Step 1: Delete existing record
            delete_params = {
                'question_id': f'eq.{question_id}',
                'language': f'eq.{language}',
                'type': f'eq.{content_type}'
            }
            client.delete(url, headers=HEADERS, params=delete_params)
            
            # Step 2: Insert new record
            data = {
                'question_id': question_id,
                'language': language,
                'type': content_type,
                'content': content
            }
            
            response = client.post(url, headers=HEADERS, json=data)
            return response.status_code in [200, 201, 204]
    except Exception as e:
        print(f"   âŒ Save error: {e}")
        return False


def format_options(options: list) -> str:
    """Format options thÃ nh chuá»—i Ä‘Ã¡nh sá»‘"""
    return '\n'.join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(options)])


def fetch_questions(start: int, end: int) -> list:
    """Láº¥y danh sÃ¡ch cÃ¢u há»i tá»« Supabase"""
    try:
        url = f"{SUPABASE_URL}/rest/v1/questions"
        params = {'select': '*'}
        
        with httpx.Client() as client:
            response = client.get(url, headers=HEADERS, params=params)
            if response.status_code != 200:
                return []
            questions = response.json()
        
        def get_number(q):
            num_str = ''.join(filter(str.isdigit, q.get('id', '')))
            return int(num_str) if num_str else 0
        
        questions.sort(key=get_number)
        
        filtered = []
        for q in questions:
            num = get_number(q)
            if start <= num <= end:
                filtered.append(q)
        return filtered
    except Exception as e:
        print(f"âŒ Error fetching questions: {e}")
        return []


def process_question(question: dict, language: str, content_types: list, force: bool = False) -> dict:
    """Xá»­ lÃ½ má»™t cÃ¢u há»i"""
    question_id = question['id']
    question_text = question['question']
    options = question['options']
    correct_answer = question['correct_answer']
    
    options_str = format_options(options)
    results = {'id': question_id, 'theory': None, 'explanation': None}
    
    for content_type in content_types:
        if not force:
            if get_cached_content(question_id, language, content_type):
                print(f"   âœ“ {content_type.capitalize()} Ä‘Ã£ cÃ³ cache, bá» qua")
                results[content_type] = 'cached'
                continue
        
        print(f"   ğŸ¤– Äang táº¡o {content_type} vá»›i Hugging Face ({HF_MODEL})...")
        
        if content_type == 'theory':
            prompt = get_theory_prompt(question_text, options_str, language)
        else:
            prompt = get_explanation_prompt(question_text, options_str, correct_answer, language)
        
        content = call_huggingface(prompt)
        
        if content:
            if save_to_cache(question_id, language, content_type, content):
                print(f"   âœ… {content_type.capitalize()} Ä‘Ã£ lÆ°u vÃ o cache")
                results[content_type] = 'success'
            else:
                results[content_type] = 'save_failed'
        else:
            results[content_type] = 'api_failed'
        
        # Hugging Face rate limits can be strict on free tier
        time.sleep(3) 
    
    return results


def main():
    parser = argparse.ArgumentParser(description='AWS AI Cache Builder (Hugging Face)')
    parser.add_argument('range', help='Range cÃ¢u há»i (VD: 1-10)')
    parser.add_argument('--lang', default='vi', choices=['vi', 'en'], help='NgÃ´n ngá»¯ (vi/en)')
    parser.add_argument('--type', choices=['theory', 'explanation'], help='Loáº¡i ná»™i dung (optional)')
    parser.add_argument('--force', action='store_true', help='Ghi Ä‘Ã¨ cache cÅ©')
    
    args = parser.parse_args()
    
    try:
        start, end = map(int, args.range.split('-'))
    except ValueError:
        print("âŒ Invalid range format. Use: start-end (e.g., 1-10)")
        sys.exit(1)
    
    print(f"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print(f"â•‘           AWS SAA-C03 AI Cache Builder (Hugging Face)        â•‘")
    print(f"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")
    print(f"â•‘  Range: {start} - {end}                                              ")
    print(f"â•‘  Language: {'Tiáº¿ng Viá»‡t' if args.lang == 'vi' else 'English'}                                      ")
    print(f"â•‘  Model: {HF_MODEL}")
    print(f"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
    
    print(f"ğŸ“š Äang láº¥y cÃ¢u há»i tá»« {start} Ä‘áº¿n {end}...")
    questions = fetch_questions(start, end)
    print(f"âœ… TÃ¬m tháº¥y {len(questions)} cÃ¢u há»i")
    
    content_types = [args.type] if args.type else ['theory', 'explanation']
    
    for i, q in enumerate(questions):
        print(f"\n[{i+1}/{len(questions)}] CÃ¢u há»i: {q['id']} ({args.lang})")
        process_question(q, args.lang, content_types, args.force)

if __name__ == "__main__":
    main()
